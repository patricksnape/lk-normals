{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Fit AAMs using the Project Out algorithm\n",
      "\n",
      "### 1. Load the data\n",
      "\n",
      "Use the **autoimporter** to automatically load the *annotated* test images of the *Labelled Faces Parts in the Wild (LFPW)* dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.io import loadmat\n",
      "from pybug.shape import PointCloud\n",
      "from pybug.io import auto_import\n",
      "from pybug.landmark.labels import ibug_68_points, ibug_68_contour, ibug_68_trimesh, labeller\n",
      "\n",
      "images = auto_import('/vol/atlas/databases/frgc/spring2004/*.abs', max_images=1)\n",
      "images = [im.as_depth_image() for im in images]\n",
      "\n",
      "# Extract shape data. -1 because the annotation are 1 based\n",
      "points = [img.landmarks['PTS'].all_landmarks.points - 1  for img in images]\n",
      "shapes = [PointCloud(p) for p in points]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use **pickle** to load a previously build *AAM*:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "\n",
      "# load a previously buid AAM\n",
      "with open('/vol/data/frgc_spring2003_cosine', \"rb\") as f:\n",
      "    aam = cPickle.load(f)\n",
      "\n",
      "reference_frame = aam[\"template\"]\n",
      "appearance_model = aam[\"appearance_model\"]\n",
      "shape_model = aam[\"shape_model\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "appearance_model._pca.mean_ = np.zeros(appearance_model.n_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. Fit\n",
      "\n",
      "Randomly select an image to fit:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.ndimage.filters import median_filter\n",
      "from pybug.image import DepthImage\n",
      "\n",
      "# select a test image at random\n",
      "ind = 0\n",
      "original_image = images[ind]\n",
      "test_shape = shapes[ind]\n",
      "\n",
      "original_image.view_new(mode='mesh', normals=original_image.mesh.vertex_normals, mask_points=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cosine_normals import logmap_cosine\n",
      "from pybug.image import MaskedNDImage\n",
      "import copy\n",
      "\n",
      "normals = original_image.mesh.vertex_normals\n",
      "features = logmap_cosine(normals)\n",
      "new_im = MaskedNDImage.blank(im.shape, mask=im.mask, n_channels=4)\n",
      "new_im.update_from_vector(features.flatten())\n",
      "new_im.landmarks.update(copy.deepcopy(im.landmarks))\n",
      "new_im._enforce_ownership_of_all_landmarks()\n",
      "\n",
      "test_image = new_im\n",
      "test_image.view_new(channel=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualize ground truth landmarks and warped appearance for the selected test image:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.align.nonrigid.tps import TPS\n",
      "from pybug.warp import scipy_warp\n",
      "\n",
      "source = reference_frame.landmarks['IBUG'].all_landmarks\n",
      "\n",
      "tps = TPS(source.points, test_shape.points)\n",
      "template = scipy_warp(test_image, reference_frame, tps.transform)\n",
      "\n",
      "test_image.add_landmark_set(\"ground_truth\", {\"ground_truth\": test_shape})\n",
      "test_image.landmarks['ground_truth'].with_label('ground_truth').view_new()\n",
      "gcf().set_size_inches((10,10))\n",
      "\n",
      "template.add_landmark_set('reference', {'reference': source})\n",
      "template.landmarks['reference'].with_label('reference').view_new()\n",
      "gcf().set_size_inches((6,6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build Statistically Driven Transform using the AAM's shape model and Piece Wise Affine (PWA) warps: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.transform.statisticallydriven import StatisticallyDrivenTransform\n",
      "from pybug.transform.affine import SimilarityTransform\n",
      "\n",
      "def tps_constructor(src_landmarks, tgt_landmarks):\n",
      "    return TPS(src_landmarks, tgt_landmarks).transform\n",
      "\n",
      "dummy_similarity_transform = SimilarityTransform.from_vector(np.array([0, 0, 0, 0]))\n",
      "global_transform = dummy_similarity_transform\n",
      "\n",
      "stat_driven_transform = StatisticallyDrivenTransform(shape_model, tps_constructor,\n",
      "                                                     source=source, \n",
      "                                                     global_transform=global_transform)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build three \"fitter\" objects using the three variations of the Project Out algorithm:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.align.lucaskanade.residual import LSIntensity\n",
      "from pybug.align.lucaskanade.appearance.projectout import (ProjectOutForwardAdditive,\n",
      "                                                           ProjectOutForwardCompositional,\n",
      "                                                           ProjectOutInverseCompositional)\n",
      "# standard Least Squares Residual\n",
      "residual = LSIntensity()\n",
      "\n",
      "# forward additive\n",
      "fa = ProjectOutForwardAdditive(appearance_model, residual, stat_driven_transform)\n",
      "# forwad compositional\n",
      "fc = ProjectOutForwardCompositional(appearance_model, residual, stat_driven_transform)\n",
      "# inverse compositional\n",
      "ic = ProjectOutInverseCompositional(appearance_model, residual, stat_driven_transform)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Initialize the fitting procedure by perturbing the true scale and translation parameters of the manually annotated shape:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.transform.affine import SimilarityTransform\n",
      "import numpy as np\n",
      "\n",
      "# estimate the true similarity transform\n",
      "similarity_transform = SimilarityTransform.estimate(shape_model.mean.points, \n",
      "                                                    test_shape.points)\n",
      "\n",
      "global_parameters = similarity_transform.as_vector()\n",
      "# kill rotation\n",
      "global_parameters[1] = 0\n",
      "# add random gaussian noise to the ground truth parameters\n",
      "global_parameters += 0.1 * np.random.randn(4) \n",
      "\n",
      "global_transform = SimilarityTransform.from_vector(global_parameters)\n",
      "\n",
      "aligned_test_points = global_transform.inverse.apply(test_shape.points)\n",
      "aligned_test_shape = PointCloud(aligned_test_points)\n",
      "weights = shape_model.project(aligned_test_shape)\n",
      "\n",
      "initial_parameters = np.concatenate([global_parameters, np.zeros_like(weights)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the correctness of the initialization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "initial_transform = stat_driven_transform.from_vector(initial_parameters)\n",
      "initial_warped_image = scipy_warp(test_image, template, initial_transform)\n",
      "\n",
      "original_image.mesh.texture.add_landmark_set(\"initial\", {\"initial\": initial_transform.target})\n",
      "original_image.mesh.texture.landmarks['initial'].view_new()\n",
      "gcf().set_size_inches((10,10))\n",
      "\n",
      "initial_warped_image.add_landmark_set('reference', {'reference': source})\n",
      "initial_warped_image.landmarks['reference'].with_label('reference').view_new()\n",
      "gcf().set_size_inches((6,6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fit the image using all three different methods:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimal_transform_1 = fa.align(test_image, initial_parameters, max_iters=50)\n",
      "optimal_transform_2 = fc.align(test_image, initial_parameters, max_iters=50)\n",
      "# optimal_transform_3 = ic.align(test_image, initial_parameters, max_iters=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualize the results:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fitted_appearance_1 = scipy_warp(test_image, template, optimal_transform_1)\n",
      "fitted_appearance_2 = scipy_warp(test_image, template, optimal_transform_2)\n",
      "# fitted_appearance_3 = scipy_warp(test_image, template, optimal_transform_3)\n",
      "\n",
      "fitted_appearance_1.add_landmark_set('reference', {'reference': source})\n",
      "fitted_appearance_1.landmarks['reference'].with_label('reference').view_new()\n",
      "gcf().set_size_inches((6,6))\n",
      "\n",
      "fitted_appearance_2.add_landmark_set('reference', {'reference': source})\n",
      "fitted_appearance_2.landmarks['reference'].with_label('reference').view_new()\n",
      "gcf().set_size_inches((6,6))\n",
      "\n",
      "# fitted_appearance_3.add_landmark_set('reference', {'reference': source})\n",
      "# fitted_appearance_3.get_landmark_set('reference').with_label('reference').view_new()\n",
      "# gcf().set_size_inches((6,6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_image.add_landmark_set('fitting_results', {'fa': optimal_transform_1.target,\n",
      "                                                'fc': optimal_transform_2.target})\n",
      "#                                                 'ic': optimal_transform_3.target})\n",
      "\n",
      "lm = test_image.landmarks['fitting_results'].with_label('fa')\n",
      "original_image.mesh.texture.landmarks['fitting_results'] = lm\n",
      "original_image.mesh.texture._enforce_ownership_of_all_landmarks()\n",
      "original_image.mesh.texture.landmarks['fitting_results'].view_new()\n",
      "gcf().set_size_inches((10,10))\n",
      "\n",
      "test_image.landmarks['fitting_results'].with_label('fc').view_new()\n",
      "gcf().set_size_inches((10,10))\n",
      "\n",
      "# test_image.get_landmark_set('fitting_results').with_label('ic').view_new()\n",
      "# gcf().set_size_inches((10,10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}